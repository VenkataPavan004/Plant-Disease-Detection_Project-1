import tensorflow as tf
import tensorflow.keras as keras
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import os
from sklearn.model_selection import train_test_split
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive', force_remount=True)

# Check GPU availability
print("Num GPUs Available:", len(tf.config.list_physical_devices('GPU')))
tf.config.optimizer.set_jit(True)  # Enable XLA
tf.keras.mixed_precision.set_global_policy('mixed_float16')

# Load dataset
dataset = pd.read_csv('/content/drive/MyDrive/train.csv')
dataset['image_id'] = dataset['image_id'] + '.jpg'

# Check for missing values
print("Missing values:", dataset.isnull().sum().sum())

# Copy images to local storage for faster access
!cp -r /content/drive/MyDrive/images /content/

# Data augmentation
image_size = (224, 224)
datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=180,
    zoom_range=0.15,
    width_shift_range=0.15,
    height_shift_range=0.15,
    horizontal_flip=True,
    vertical_flip=True,
    preprocessing_function=tf.keras.applications.xception.preprocess_input
)

# Train-validation split
X_train, X_valid = train_test_split(dataset, test_size=0.05, random_state=42)
BATCH_SIZE = 16

# Data generators
train_generator = datagen.flow_from_dataframe(
    X_train,
    directory='/content/images',
    x_col='image_id',
    y_col=['healthy', 'multiple_diseases', 'rust', 'scab'],
    target_size=image_size,
    class_mode='raw',
    batch_size=BATCH_SIZE,
    shuffle=True
)

valid_generator = datagen.flow_from_dataframe(
    X_valid,
    directory='/content/images',
    x_col='image_id',
    y_col=['healthy', 'multiple_diseases', 'rust', 'scab'],
    target_size=image_size,
    class_mode='raw',
    batch_size=BATCH_SIZE,
    shuffle=False
)

# Define model
inputs = tf.keras.Input(shape=(224, 224, 3))
xception_model = tf.keras.applications.Xception(include_top=False, weights='imagenet', input_tensor=inputs)
densenet_model = tf.keras.applications.DenseNet121(include_top=False, weights='imagenet', input_tensor=inputs)

xception_output = tf.keras.layers.GlobalAveragePooling2D()(xception_model.output)
densenet_output = tf.keras.layers.GlobalAveragePooling2D()(densenet_model.output)

# Ensure same output size before concatenation
xception_output = tf.keras.layers.Dense(1024, activation='relu')(xception_output)
densenet_output = tf.keras.layers.Dense(1024, activation='relu')(densenet_output)

# Concatenate the outputs
combined = tf.keras.layers.Concatenate()([xception_output, densenet_output])

# Final output layer
outputs = tf.keras.layers.Dense(4, activation='softmax', dtype='float32')(combined)

# Build model
model = tf.keras.Model(inputs, outputs)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# Callbacks
lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, verbose=1)
model_checkpoint = tf.keras.callbacks.ModelCheckpoint('best_model.h5', monitor='val_accuracy', save_best_only=True, verbose=1)

# Train Model
EPOCHS = 20
history = model.fit(
    train_generator,
    epochs=EPOCHS,
    validation_data=valid_generator,
    callbacks=[model_checkpoint, lr_callback]
)

# Save Model History
pd.DataFrame(history.history).to_csv('ModelHistory.csv')

# Plot Training Progress
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.legend()
plt.title('Accuracy Plot')

plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.legend()
plt.title('Loss Plot')
plt.show()

# Load Test Data
test_dataset = pd.read_csv('/content/drive/MyDrive/test.csv')
test_dataset['image_id'] = test_dataset['image_id'] + '.jpg'

# Test Data Generator
test_gen = datagen.flow_from_dataframe(
    test_dataset,
    directory='/content/images',
    x_col='image_id',
    target_size=image_size,
    class_mode=None,
    shuffle=False,
    batch_size=BATCH_SIZE
)

# Make Predictions
predictions = model.predict(test_gen)
